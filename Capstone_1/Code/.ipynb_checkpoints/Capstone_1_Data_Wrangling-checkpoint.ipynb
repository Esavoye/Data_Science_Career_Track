{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Springboard--DSC Program\n",
    "\n",
    "# Capstone Project 1 - Data Wrangling \n",
    "### by Ellen A. Savoye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data collected is from a Kaggle competition, Jigsaw Unintended Bias in Toxicity Classification, via the Kaggle API. Of the 7 files in the zipped data, we will be focusing on the 'train' data. The original 'train' data is comprised of 45 columns containing information on toxicity and identity labels, comments, and metadata. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kaggle\n",
    "# !pip install spacy\n",
    "# !pip install spacymoji\n",
    "# !pip install emot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# libraries for NLP\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from contractions import CONTRACTION_MAP\n",
    "import unicodedata\n",
    "import string\n",
    "from emot.emo_unicode import UNICODE_EMO, EMOTICONS\n",
    "\n",
    "# libraries for getting and moving data\n",
    "import os\n",
    "from os import path\n",
    "import shutil\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary dependencies for text pre-processing\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', parse=True, tag=True, entity=True)\n",
    "#nlp_vec = spacy.load('en_vecs', parse = True, tag=True, #entity=True)\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is already downloaded\n"
     ]
    }
   ],
   "source": [
    "# Set directories\n",
    "src = \"C:\\\\Users\\\\ellen\\\\Documents\\\\GitHub\\\\Data_Science_Career_Track\\\\Capstone_1\\\\Code\\\\\"\n",
    "dst = \"C:\\\\Users\\\\ellen\\\\Documents\\\\GitHub\\\\Data_Science_Career_Track\\\\Capstone_1\\\\Data\\\\\"\n",
    "\n",
    "kaggle_comp_name = 'jigsaw-unintended-bias-in-toxicity-classification'\n",
    "zipfile_name = kaggle_comp_name + '.zip'\n",
    "\n",
    "csv_file = [i for i in os.listdir(dst) if i.startswith(\"train\") and path.isfile(path.join(dst, i))]\n",
    "zip_file = [i for i in os.listdir(dst) if i.startswith(\"jigsaw\") and path.isfile(path.join(dst, i))]\n",
    "\n",
    "if zip_file[0] != zipfile_name:\n",
    "    #Import data from Kaggle API\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    files = api.competition_download_files(kaggle_comp_name)\n",
    "    \n",
    "    # Move the jigsaw zip file to the Data folder\n",
    "    files = [i for i in os.listdir(src) if i.startswith(\"jigsaw\") and path.isfile(path.join(src, i))]\n",
    "    for f in files:\n",
    "        shutil.move(path.join(src, f), dst)\n",
    "    \n",
    "    # Check if Train data is already extracted\n",
    "    if csv_file != 'train.csv':\n",
    "        with ZipFile(dst + zipfile_name, 'r') as zipObj:\n",
    "            # Extract all the contents of zip file in current directory\n",
    "            print(zipObj.namelist())\n",
    "            zipObj.extract('train.csv', path = dst)\n",
    "else:\n",
    "    print('Data is already downloaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 1804874\n",
      "['id', 'target', 'comment_text', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat', 'asian', 'atheist', 'bisexual', 'black', 'buddhist', 'christian', 'female', 'heterosexual', 'hindu', 'homosexual_gay_or_lesbian', 'intellectual_or_learning_disability', 'jewish', 'latino', 'male', 'muslim', 'other_disability', 'other_gender', 'other_race_or_ethnicity', 'other_religion', 'other_sexual_orientation', 'physical_disability', 'psychiatric_or_mental_illness', 'transgender', 'white', 'created_date', 'publication_id', 'parent_id', 'article_id', 'rating', 'funny', 'wow', 'sad', 'likes', 'disagree', 'sexual_explicit', 'identity_annotator_count', 'toxicity_annotator_count']\n"
     ]
    }
   ],
   "source": [
    "# Read in the train dataset\n",
    "csv_filename = 'train.csv'\n",
    "train_data = pd.read_csv(dst + csv_filename, low_memory=False)\n",
    "\n",
    "# Output the number of rows\n",
    "print(\"Total rows: {0}\".format(len(train_data)))\n",
    "\n",
    "# See which headers are available\n",
    "print(list(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metadata from Civil Comments platform is contained in the following columns: created_date, publication_id, parent_id, article_id, rating, funny, wow, sad, likes, and disagree.\n",
    "\n",
    "I will be keeping these fields in for further exploration and possible use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for counting null records:\n",
    "def num_missing(x):\n",
    "    return sum(x.isnull())\n",
    "\n",
    "#Applying per column:\n",
    "print(\"Missing values per column:\")\n",
    "print(train_data.apply(num_missing, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find percent of missing values for each column instead of number of records\n",
    "\n",
    "percent_missing = train_data.isnull().sum() * 100 / len(train_data)\n",
    "print(round(percent_missing,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data type of each column\n",
    "\n",
    "train_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.rating.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1804874, 45)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 45 columns in the train dataframe. Of those 45, only three columns, 'comment_text', 'created_date', and 'rating', are objects. The remaining 42 are either float64 or int64. These columns are non-categorical. 'Comment_text' is categorical containing the individual comments that we need to analyze. 'Created_date' contains the original date the comments were created. 'Rating' is a categorical containing two values: rejected or approved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.comment_text.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View unique records in a particular column\n",
    "\n",
    "#sorted(train_data.target.unique())\n",
    "train_data.target.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toxicity and identity labels range from 0.0-1.0. The value represents the fraction of raters who believed the label fit the comment. Toxicity labels do not have any missing values. According to the competition details, a subset of comments have been labeled with a variety of identity attributes that have been mentioned in the comment. As such, every identity label is missing ~78% of the values per column. The subset comprises approximately 22% of the data. \n",
    "\n",
    "Two examples of how labeling works are as follows:\n",
    "Example 1: \n",
    "    - Comment: I'm a white woman in my late 60's and believe me, they are not too crazy about me either!!\n",
    "    - Toxicity Labels: All 0.0\n",
    "    - Identity Mention Labels: female: 1.0, white: 1.0 (all others 0.0)\n",
    "\n",
    "Example 2: \n",
    "    - Comment: Continue to stand strong LGBT community. Yes, indeed, you'll overcome and you have.\n",
    "    - Toxicity Labels: All 0.0\n",
    "    - Identity Mention Labels: homosexual_gay_or_lesbian: 0.8, bisexual: 0.6, transgender: 0.3 (all others 0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Target' is the toxicity label. 'Severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat', and 'sexual_explicit' are toxicity sub types. All toxicity labels can be converted to categorical variables by using >= 0.5 as a positive indicator (1). \n",
    "\n",
    "Aside from 'id', 'comment_text', 'identity_annotator_count' and 'toxicity_annotator_count', the same conversion can be applied to the remaining identity columns. 'Id' is a unique identifier for each comment but may not hold value to keep in the data frame. 'Identity_annotator_count' and 'toxicity_annotator_count' are metadata columns from Jigsaw and may not hold value either. However, I'm not removing them until I do my exploratory data analysis to determine if they offer valuable insights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Comment_text' will need to be cleaned, vectorized, and eventually create a design matrix.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for blank string records\n",
    "np.where(train_data.applymap(lambda x: x == ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the range of numerical columns\n",
    "print('Minimum value: ')\n",
    "train_data.iloc[:,:].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Maximum value: ')\n",
    "train_data.iloc[:,:].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what can be deleted from messages field?\n",
    "-stop words, punctuation, emoji, \n",
    "\n",
    "lematization or stemming\n",
    "output of wrangling - corpus can be part of a data frame with other cleaned up columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Why removing meta-data columns? There has to be a data-science driven reason ...\n",
    "    1) can't delete columns from dataset without approval from owner of data\n",
    "    2) just because I don't see a connection doesn't mean there is one\n",
    "    3) how can use the information that we're given to create a potential feature that is a measure the level of intention - like or don't like\n",
    "\n",
    "- Null values implies values that are missing. Is it possible to have values that are missing that are not null?\n",
    "    1) cases where values are not null but can still be not good\n",
    " \n",
    "- How would we check for data quality?\n",
    "    1) max, min, range, content, etc.\n",
    "\n",
    "- There seem to be many columns with about 80% of values that are missing. How do we deal with this?\n",
    "    1) Could limit the data to the ~20% of the data with identifiers\n",
    "    2) concentrate on comment_text and response variable initially?\n",
    "    \n",
    "    a) would it make sense to compute them myself?\n",
    "\n",
    "- Do we know what columns are categorical and what columns are non-categorical? \n",
    "    1) I would say the identity labels are categorical\n",
    "    2) target is the response\n",
    "    3) what would comment_text be?\n",
    "    non-categorical predominantly - continuous metric\n",
    "    categorical - comment_text, refers to a class\n",
    "\n",
    "- Think about ways of persisting the dataset as an output of the wrangling phase, which will be an input to the next phase storytelling). (Hint ... you can use something like Pickle, or perhaps generate a CSV file)\n",
    "    1) pickle the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "emoji - encoded to a string as a token (possibility); do search on how to deal with emoji in NLP\n",
    "\n",
    "packages - nltk, spacy\n",
    "\n",
    "check if summation of identity labels across rows is equal to 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unicode remove, freq count is an option, every emoji maps to a unicode character and maybe can map it to a text entity. \n",
    "\n",
    "\n",
    "frequency stopwords, manual inspection\n",
    "think about & determine - punc, stop words, conjunctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the size of the original dataset, ~1.8M records, I took a 30% random sample to quantify and test some of the text cleaning and pre-processing ideas: testing for punctuation, emoticons, emoji, accented words, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a smaller 30% random sample data frame to test text cleaning/pre-processing\n",
    "\n",
    "subset_train_data = train_data.sample(frac=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(541462, 45)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the emojis\n",
    "emojis_list = map(lambda x: ''.join(x.split()), UNICODE_EMO.keys())\n",
    "r = re.compile('|'.join(re.escape(p) for p in emojis_list))\n",
    "#English\n",
    "subset_train_data['emoji'] = subset_train_data['comment_text'].str.findall(r)\n",
    "#Emoji count\n",
    "subset_train_data['emoji_count'] = subset_train_data['emoji'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the emoticons\n",
    "emoticons_list = map(lambda x: ''.join(x.split()), EMOTICONS.keys())\n",
    "r = re.compile('|'.join(re.escape(p) for p in emoticons_list))\n",
    "#English\n",
    "subset_train_data['emoticons'] = subset_train_data['comment_text'].str.findall(r)\n",
    "#Emoji count\n",
    "subset_train_data['emoticons_count'] = subset_train_data['emoticons'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoticons percentage in sample data:  4.1238 %\n"
     ]
    }
   ],
   "source": [
    "print('Emoticons percentage in sample data: ', round(subset_train_data['emoticons_count'].sum() / subset_train_data['emoticons_count'].count() * 100,4), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoji percentage in sample data:  0.4482 %\n"
     ]
    }
   ],
   "source": [
    "print('Emoji percentage in sample data: ', round(subset_train_data['emoji_count'].sum() / subset_train_data['emoji_count'].count() * 100,4), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My sample data contains emoticons in approximately 4% of the data and emoji in .44%. I'm going to split each comment into a list of words before removing punctuation to preserve the integrity of contracted words, i.e. we're or won't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{33: None, 34: None, 35: None, 36: None, 37: None, 38: None, 39: None, 40: None, 41: None, 42: None, 43: None, 44: None, 45: None, 46: None, 47: None, 58: None, 59: None, 60: None, 61: None, 62: None, 63: None, 64: None, 91: None, 92: None, 93: None, 94: None, 95: None, 96: None, 123: None, 124: None, 125: None, 126: None}\n"
     ]
    }
   ],
   "source": [
    "# print(string.punctuation)\n",
    "\n",
    "\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# split into words by white space\n",
    "subset_train_data.loc[:,'comment_text_split'] = subset_train_data.loc[:,'comment_text'].apply(lambda x: x.split())\n",
    "# remove punctuation from each word\n",
    "\n",
    "table = str.maketrans('', '', string.punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1103467    [Spend, time, in, the, parks, and, with, the, ...\n",
       "1682004    [I, would, offer, a, rebuttal, to, your, opini...\n",
       "1313804    [And, while, we're, waiting, for, her, to, be,...\n",
       "972595     [Really?, Where, has, it, been, so, 'incorrect...\n",
       "1803912    [Of, course, it's, a, government, mandate., It...\n",
       "Name: comment_text_split, dtype: object"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_train_data.comment_text_split.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to strip punctuation\n",
    "import string\n",
    "from string import punctuation\n",
    "\n",
    "def strip_punctuation(s):\n",
    "    return ''.join(c for c in s if c not in punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_train_data.loc[:,'comment_text_punct'] = subset_train_data.loc[:,'comment_text'].apply(lambda x: strip_punctuation(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1103467    Spend time in the parks and with the rangers a...\n",
       "1682004    I would offer a rebuttal to your opinion  But ...\n",
       "1313804    And while were waiting for her to be found or ...\n",
       "972595     Really  Where has it been so incorrect  Candid...\n",
       "1803912    Of course its a government mandate Its governm...\n",
       "Name: comment_text_punct, dtype: object"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_train_data.comment_text_punct.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the emojis\n",
    "emojis_list = map(lambda x: ''.join(x.split()), UNICODE_EMO.keys())\n",
    "r = re.compile('|'.join(re.escape(p) for p in emojis_list))\n",
    "#English\n",
    "subset_train_data['emoji'] = subset_train_data['comment_text_punct'].str.findall(r)\n",
    "#Emoji count\n",
    "subset_train_data['emoji_count'] = subset_train_data['emoji'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the emoticons\n",
    "emoticons_list = map(lambda x: ''.join(x.split()), EMOTICONS.keys())\n",
    "r = re.compile('|'.join(re.escape(p) for p in emoticons_list))\n",
    "#English\n",
    "subset_train_data['emoticons'] = subset_train_data['comment_text_punct'].str.findall(r)\n",
    "#Emoji count\n",
    "subset_train_data['emoticons_count'] = subset_train_data['emoticons'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoticons percentage in sample data:  0.0837 %\n"
     ]
    }
   ],
   "source": [
    "print('Emoticons percentage in sample data: ', round(subset_train_data['emoticons_count'].sum() / subset_train_data['emoticons_count'].count() * 100,4), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoji percentage in sample data:  0.4482 %\n"
     ]
    }
   ],
   "source": [
    "print('Emoji percentage in sample data: ', round(subset_train_data['emoji_count'].sum() / subset_train_data['emoji_count'].count() * 100,4), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_train_data_v2 = train_data.sample(frac=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36097, 45)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_train_data_v2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2235681\n"
     ]
    }
   ],
   "source": [
    "# Count non-alphanumerical characters in sample\n",
    "count_special = 0\n",
    "for line in subset_train_data_v2['comment_text']:\n",
    "    count_special += sum(not x.isalnum() for x in line)\n",
    "print(count_special)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of punctuation characters exists in string: \n",
      "320276\n"
     ]
    }
   ],
   "source": [
    "# Quantify '!', \",\" ,\"\\'\" ,\";\" ,\"\\\"\", \".\", \"-\" ,\"?\" in sample data\n",
    "count = 0\n",
    "for line in subset_train_data_v2['comment_text']:\n",
    "    count += sum(1 for i in line if i in ('!', \",\" ,\"\\'\" ,\";\" ,\"\\\"\", \".\", \"-\" ,\"?\"))\n",
    "          \n",
    "print(\"Total number of punctuation characters exists in string: \")\n",
    "print(count);  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attemp to check for accented characters\n",
    "\n",
    "def count_ascii(text):\n",
    "    if text.isascii() == False:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "subset_train_data.loc[:,'comment_text_ascii'] = subset_train_data.loc[:,'comment_text'].apply(lambda x: count_accented_chars(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset_train_data_v2.loc[:,['comment_text','comment_text_ascii']].to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accented characters percentage in sample data:  100.0 %\n"
     ]
    }
   ],
   "source": [
    "print('Accented characters percentage in sample data: ', round(subset_train_data['comment_text_ascii'].sum() / subset_train_data['comment_text_ascii'].count() * 100,4), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create comment field in all lower case\n",
    "\n",
    "train_data['comment_lower'] = train_data['comment_text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['comment_lower'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of removing emojis or emoticons, we'll convert them into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert emoji and emoticons to words\n",
    "\n",
    "def convert_emojis(text):\n",
    "    for emot in UNICODE_EMO:\n",
    "        text = text.replace(emot, \"_\".join(UNICODE_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "    return text\n",
    "# Converting emoticons to words    \n",
    "def convert_emoticons(text):\n",
    "    for emot in EMOTICONS:\n",
    "        text = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").split()), text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new comment field with converted emoji and emoticon\n",
    "\n",
    "train_data['comment_transform'] = train_data['comment_lower'].apply(convert_emoticons)\n",
    "train_data['comment_transform'] = train_data['comment_lower'].apply(convert_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['comment_transform'].max()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove accented characters - convert to standardized ASCII characters\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "train_data.loc[:,'comment_text_cleaned'] = train_data.loc[:,'comment_text'].apply(lambda x: remove_accented_chars(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing contractions \n",
    "\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "train_data.loc[:,'comment_text_cleaned'] = train_data.loc[:,'comment_text_cleaned'].apply(lambda x: expand_contractions(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on results of the model, may need to go back and do more cleaning.\n",
    "\n",
    "order of complexity\n",
    "-stopwords and punctuation with NLTK\n",
    "    - be careful with apostrophe. ex - didn't -> didnt as a token (wont need to worry about contractions)\n",
    "    - if by taking care of punctuation, could take care of contractions (don't split)\n",
    "    - think about by-product\n",
    "    \n",
    "- accented words\n",
    "    - do we have a ratio or amount? is it worth while to do? \n",
    "    \n",
    "- extract count of emoji/emoticons from text\n",
    "    what proportion is it?\n",
    "    \n",
    "-lemmitization or stemming, one not both\n",
    "\n",
    "-spelling\n",
    "    -is it quantifiable; compare against a dictionary? not important for the sake of spelling but for consistency across the corpus\n",
    "    \n",
    "- split into a more manageable size to do my experimentation then apply to full data (random sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pickle file for use in the future\n",
    "\n",
    "train_data.to_pickle(dst + '/cleaned_train_data.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
