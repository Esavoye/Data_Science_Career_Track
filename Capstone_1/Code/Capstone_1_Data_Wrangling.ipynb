{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Springboard--DSC Program\n",
    "\n",
    "# Capstone Project 1 - Data Wrangling \n",
    "### by Ellen A. Savoye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data collected is from a Kaggle competition, Jigsaw Unintended Bias in Toxicity Classification, via the Kaggle API. Of the 7 files in the zipped data, we will be focusing on the 'train' data. The original 'train' data is comprised of 45 columns containing information on toxicity and identity labels, comments, and metadata. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kaggle\n",
    "# !pip install spacy\n",
    "# !pip install spacymoji\n",
    "# !pip install emot\n",
    "# !pip install demoji\n",
    "# !pip install swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDownloading emoji data ...\u001b[0m\n",
      "\u001b[92m... OK\u001b[0m (Got response in 0.38 seconds)\n",
      "\u001b[33mWriting emoji data to C:\\Users\\ellen\\.demoji/codes.json ...\u001b[0m\n",
      "\u001b[92m... OK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# libraries for NLP\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from contractions import CONTRACTION_MAP\n",
    "import unicodedata\n",
    "import string\n",
    "from emot.emo_unicode import UNICODE_EMO, EMOTICONS\n",
    "import demoji\n",
    "demoji.download_codes()\n",
    "import swifter\n",
    "\n",
    "# libraries for getting and moving data\n",
    "import os\n",
    "from os import path\n",
    "import shutil\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary dependencies for text pre-processing\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', parse=True, tag=True, entity=True)\n",
    "#nlp_vec = spacy.load('en_vecs', parse = True, tag=True, #entity=True)\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is already downloaded\n"
     ]
    }
   ],
   "source": [
    "# Set directories\n",
    "src = \"C:\\\\Users\\\\ellen\\\\Documents\\\\GitHub\\\\Data_Science_Career_Track\\\\Capstone_1\\\\Code\\\\\"\n",
    "dst = \"C:\\\\Users\\\\ellen\\\\Documents\\\\GitHub\\\\Data_Science_Career_Track\\\\Capstone_1\\\\Data\\\\\"\n",
    "\n",
    "kaggle_comp_name = 'jigsaw-unintended-bias-in-toxicity-classification'\n",
    "zipfile_name = kaggle_comp_name + '.zip'\n",
    "\n",
    "csv_file = [i for i in os.listdir(dst) if i.startswith(\"train\") and path.isfile(path.join(dst, i))]\n",
    "zip_file = [i for i in os.listdir(dst) if i.startswith(\"jigsaw\") and path.isfile(path.join(dst, i))]\n",
    "\n",
    "if zip_file[0] != zipfile_name:\n",
    "    #Import data from Kaggle API\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    files = api.competition_download_files(kaggle_comp_name)\n",
    "    \n",
    "    # Move the jigsaw zip file to the Data folder\n",
    "    files = [i for i in os.listdir(src) if i.startswith(\"jigsaw\") and path.isfile(path.join(src, i))]\n",
    "    for f in files:\n",
    "        shutil.move(path.join(src, f), dst)\n",
    "    \n",
    "    # Check if Train data is already extracted\n",
    "    if csv_file != 'train.csv':\n",
    "        with ZipFile(dst + zipfile_name, 'r') as zipObj:\n",
    "            # Extract all the contents of zip file in current directory\n",
    "            print(zipObj.namelist())\n",
    "            zipObj.extract('train.csv', path = dst)\n",
    "else:\n",
    "    print('Data is already downloaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 1804874\n",
      "['id', 'target', 'comment_text', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat', 'asian', 'atheist', 'bisexual', 'black', 'buddhist', 'christian', 'female', 'heterosexual', 'hindu', 'homosexual_gay_or_lesbian', 'intellectual_or_learning_disability', 'jewish', 'latino', 'male', 'muslim', 'other_disability', 'other_gender', 'other_race_or_ethnicity', 'other_religion', 'other_sexual_orientation', 'physical_disability', 'psychiatric_or_mental_illness', 'transgender', 'white', 'created_date', 'publication_id', 'parent_id', 'article_id', 'rating', 'funny', 'wow', 'sad', 'likes', 'disagree', 'sexual_explicit', 'identity_annotator_count', 'toxicity_annotator_count']\n"
     ]
    }
   ],
   "source": [
    "# Read in the train dataset\n",
    "csv_filename = 'train.csv'\n",
    "train_data = pd.read_csv(dst + csv_filename, low_memory=False)\n",
    "\n",
    "# Output the number of rows\n",
    "print(\"Total rows: {0}\".format(len(train_data)))\n",
    "\n",
    "# See which headers are available\n",
    "print(list(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metadata from Civil Comments platform is contained in the following columns: created_date, publication_id, parent_id, article_id, rating, funny, wow, sad, likes, and disagree.\n",
    "\n",
    "I will be keeping these fields in for further exploration and possible use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "id                                           0\n",
      "target                                       0\n",
      "comment_text                                 0\n",
      "severe_toxicity                              0\n",
      "obscene                                      0\n",
      "identity_attack                              0\n",
      "insult                                       0\n",
      "threat                                       0\n",
      "asian                                  1399744\n",
      "atheist                                1399744\n",
      "bisexual                               1399744\n",
      "black                                  1399744\n",
      "buddhist                               1399744\n",
      "christian                              1399744\n",
      "female                                 1399744\n",
      "heterosexual                           1399744\n",
      "hindu                                  1399744\n",
      "homosexual_gay_or_lesbian              1399744\n",
      "intellectual_or_learning_disability    1399744\n",
      "jewish                                 1399744\n",
      "latino                                 1399744\n",
      "male                                   1399744\n",
      "muslim                                 1399744\n",
      "other_disability                       1399744\n",
      "other_gender                           1399744\n",
      "other_race_or_ethnicity                1399744\n",
      "other_religion                         1399744\n",
      "other_sexual_orientation               1399744\n",
      "physical_disability                    1399744\n",
      "psychiatric_or_mental_illness          1399744\n",
      "transgender                            1399744\n",
      "white                                  1399744\n",
      "created_date                                 0\n",
      "publication_id                               0\n",
      "parent_id                               778646\n",
      "article_id                                   0\n",
      "rating                                       0\n",
      "funny                                        0\n",
      "wow                                          0\n",
      "sad                                          0\n",
      "likes                                        0\n",
      "disagree                                     0\n",
      "sexual_explicit                              0\n",
      "identity_annotator_count                     0\n",
      "toxicity_annotator_count                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#function for counting null records:\n",
    "def num_missing(x):\n",
    "    return sum(x.isnull())\n",
    "\n",
    "#Applying per column:\n",
    "print(\"Missing values per column:\")\n",
    "print(train_data.apply(num_missing, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                                      0.0\n",
      "target                                  0.0\n",
      "comment_text                            0.0\n",
      "severe_toxicity                         0.0\n",
      "obscene                                 0.0\n",
      "identity_attack                         0.0\n",
      "insult                                  0.0\n",
      "threat                                  0.0\n",
      "asian                                  77.6\n",
      "atheist                                77.6\n",
      "bisexual                               77.6\n",
      "black                                  77.6\n",
      "buddhist                               77.6\n",
      "christian                              77.6\n",
      "female                                 77.6\n",
      "heterosexual                           77.6\n",
      "hindu                                  77.6\n",
      "homosexual_gay_or_lesbian              77.6\n",
      "intellectual_or_learning_disability    77.6\n",
      "jewish                                 77.6\n",
      "latino                                 77.6\n",
      "male                                   77.6\n",
      "muslim                                 77.6\n",
      "other_disability                       77.6\n",
      "other_gender                           77.6\n",
      "other_race_or_ethnicity                77.6\n",
      "other_religion                         77.6\n",
      "other_sexual_orientation               77.6\n",
      "physical_disability                    77.6\n",
      "psychiatric_or_mental_illness          77.6\n",
      "transgender                            77.6\n",
      "white                                  77.6\n",
      "created_date                            0.0\n",
      "publication_id                          0.0\n",
      "parent_id                              43.1\n",
      "article_id                              0.0\n",
      "rating                                  0.0\n",
      "funny                                   0.0\n",
      "wow                                     0.0\n",
      "sad                                     0.0\n",
      "likes                                   0.0\n",
      "disagree                                0.0\n",
      "sexual_explicit                         0.0\n",
      "identity_annotator_count                0.0\n",
      "toxicity_annotator_count                0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Find percent of missing values for each column instead of number of records\n",
    "\n",
    "percent_missing = train_data.isnull().sum() * 100 / len(train_data)\n",
    "print(round(percent_missing,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                       int64\n",
       "target                                 float64\n",
       "comment_text                            object\n",
       "severe_toxicity                        float64\n",
       "obscene                                float64\n",
       "identity_attack                        float64\n",
       "insult                                 float64\n",
       "threat                                 float64\n",
       "asian                                  float64\n",
       "atheist                                float64\n",
       "bisexual                               float64\n",
       "black                                  float64\n",
       "buddhist                               float64\n",
       "christian                              float64\n",
       "female                                 float64\n",
       "heterosexual                           float64\n",
       "hindu                                  float64\n",
       "homosexual_gay_or_lesbian              float64\n",
       "intellectual_or_learning_disability    float64\n",
       "jewish                                 float64\n",
       "latino                                 float64\n",
       "male                                   float64\n",
       "muslim                                 float64\n",
       "other_disability                       float64\n",
       "other_gender                           float64\n",
       "other_race_or_ethnicity                float64\n",
       "other_religion                         float64\n",
       "other_sexual_orientation               float64\n",
       "physical_disability                    float64\n",
       "psychiatric_or_mental_illness          float64\n",
       "transgender                            float64\n",
       "white                                  float64\n",
       "created_date                            object\n",
       "publication_id                           int64\n",
       "parent_id                              float64\n",
       "article_id                               int64\n",
       "rating                                  object\n",
       "funny                                    int64\n",
       "wow                                      int64\n",
       "sad                                      int64\n",
       "likes                                    int64\n",
       "disagree                                 int64\n",
       "sexual_explicit                        float64\n",
       "identity_annotator_count                 int64\n",
       "toxicity_annotator_count                 int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data type of each column\n",
    "\n",
    "train_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['rejected', 'approved'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.rating.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1804874, 45)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 45 columns in the train dataframe. Of those 45, only three columns, 'comment_text', 'created_date', and 'rating', are objects. The remaining 42 are either float64 or int64. These columns are non-categorical. 'Comment_text' is categorical containing the individual comments that we need to analyze. 'Created_date' contains the original date the comments were created. 'Rating' is a categorical containing two values: rejected or approved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    This is so cool. It's like, 'would you want yo...\n",
       "1    Thank you!! This would make my life a lot less...\n",
       "2    This is such an urgent design problem; kudos t...\n",
       "3    Is this something I'll be able to install on m...\n",
       "4                 haha you guys are a bunch of losers.\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.comment_text.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.89361702, 0.66666667, ..., 0.87726476, 0.01116838,\n",
       "       0.87008821])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View unique records in a particular column\n",
    "\n",
    "#sorted(train_data.target.unique())\n",
    "train_data.target.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64), array([], dtype=int64))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for blank string records\n",
    "np.where(train_data.applymap(lambda x: x == ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum value: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id                                                                                 59848\n",
       "target                                                                                 0\n",
       "comment_text                           \u0010Canada is north of the USA border,  its colde...\n",
       "severe_toxicity                                                                        0\n",
       "obscene                                                                                0\n",
       "identity_attack                                                                        0\n",
       "insult                                                                                 0\n",
       "threat                                                                                 0\n",
       "asian                                                                                  0\n",
       "atheist                                                                                0\n",
       "bisexual                                                                               0\n",
       "black                                                                                  0\n",
       "buddhist                                                                               0\n",
       "christian                                                                              0\n",
       "female                                                                                 0\n",
       "heterosexual                                                                           0\n",
       "hindu                                                                                  0\n",
       "homosexual_gay_or_lesbian                                                              0\n",
       "intellectual_or_learning_disability                                                    0\n",
       "jewish                                                                                 0\n",
       "latino                                                                                 0\n",
       "male                                                                                   0\n",
       "muslim                                                                                 0\n",
       "other_disability                                                                       0\n",
       "other_gender                                                                           0\n",
       "other_race_or_ethnicity                                                                0\n",
       "other_religion                                                                         0\n",
       "other_sexual_orientation                                                               0\n",
       "physical_disability                                                                    0\n",
       "psychiatric_or_mental_illness                                                          0\n",
       "transgender                                                                            0\n",
       "white                                                                                  0\n",
       "created_date                                               2015-09-29 10:50:41.987077+00\n",
       "publication_id                                                                         2\n",
       "parent_id                                                                          61006\n",
       "article_id                                                                          2006\n",
       "rating                                                                          approved\n",
       "funny                                                                                  0\n",
       "wow                                                                                    0\n",
       "sad                                                                                    0\n",
       "likes                                                                                  0\n",
       "disagree                                                                               0\n",
       "sexual_explicit                                                                        0\n",
       "identity_annotator_count                                                               0\n",
       "toxicity_annotator_count                                                               3\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the range of numerical columns\n",
    "print('Minimum value: ')\n",
    "train_data.iloc[:,:].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum value: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id                                                           6334010\n",
       "target                                                             1\n",
       "comment_text                                         ü§£gotta love it!\n",
       "severe_toxicity                                                    1\n",
       "obscene                                                            1\n",
       "identity_attack                                                    1\n",
       "insult                                                             1\n",
       "threat                                                             1\n",
       "asian                                                              1\n",
       "atheist                                                            1\n",
       "bisexual                                                           1\n",
       "black                                                              1\n",
       "buddhist                                                           1\n",
       "christian                                                          1\n",
       "female                                                             1\n",
       "heterosexual                                                       1\n",
       "hindu                                                              1\n",
       "homosexual_gay_or_lesbian                                          1\n",
       "intellectual_or_learning_disability                                1\n",
       "jewish                                                             1\n",
       "latino                                                             1\n",
       "male                                                               1\n",
       "muslim                                                             1\n",
       "other_disability                                                 0.6\n",
       "other_gender                                                    0.75\n",
       "other_race_or_ethnicity                                            1\n",
       "other_religion                                                     1\n",
       "other_sexual_orientation                                        0.75\n",
       "physical_disability                                                1\n",
       "psychiatric_or_mental_illness                                      1\n",
       "transgender                                                        1\n",
       "white                                                              1\n",
       "created_date                           2017-11-11 01:01:10.822969+00\n",
       "publication_id                                                   115\n",
       "parent_id                                                6.33396e+06\n",
       "article_id                                                    399541\n",
       "rating                                                      rejected\n",
       "funny                                                            102\n",
       "wow                                                               21\n",
       "sad                                                               31\n",
       "likes                                                            300\n",
       "disagree                                                         187\n",
       "sexual_explicit                                                    1\n",
       "identity_annotator_count                                        1866\n",
       "toxicity_annotator_count                                        4936\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Maximum value: ')\n",
    "train_data.iloc[:,:].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toxicity and identity labels range from 0.0-1.0. The value represents the fraction of raters who believed the label fit the comment. Toxicity labels do not have any missing values. According to the competition details, a subset of comments have been labeled with a variety of identity attributes that have been mentioned in the comment. As such, every identity label is missing ~78% of the values per column. The subset comprises approximately 22% of the data. \n",
    "\n",
    "Two examples of how labeling works are as follows:\n",
    "Example 1: \n",
    "    - Comment: I'm a white woman in my late 60's and believe me, they are not too crazy about me either!!\n",
    "    - Toxicity Labels: All 0.0\n",
    "    - Identity Mention Labels: female: 1.0, white: 1.0 (all others 0.0)\n",
    "\n",
    "Example 2: \n",
    "    - Comment: Continue to stand strong LGBT community. Yes, indeed, you'll overcome and you have.\n",
    "    - Toxicity Labels: All 0.0\n",
    "    - Identity Mention Labels: homosexual_gay_or_lesbian: 0.8, bisexual: 0.6, transgender: 0.3 (all others 0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Target' is the toxicity label. 'Severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat', and 'sexual_explicit' are toxicity sub types. All toxicity labels can be converted to categorical variables by using >= 0.5 as a positive indicator (1). \n",
    "\n",
    "Aside from 'id', 'comment_text', 'identity_annotator_count' and 'toxicity_annotator_count', the same conversion can be applied to the remaining identity columns. 'Id' is a unique identifier for each comment but may not hold value to keep in the data frame. 'Identity_annotator_count' and 'toxicity_annotator_count' are metadata columns from Jigsaw and may not hold value either. However, I'm not removing them until I do my exploratory data analysis to determine if they offer valuable insights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pre-processing and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the size of the original dataset, ~1.8M records, I took a 25% random sample to quantify and test some of the text cleaning and pre-processing ideas: testing for punctuation, emoticons, emoji, accented words, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a smaller 25% random sample data frame to test text cleaning/pre-processing\n",
    "\n",
    "subset_train_data = train_data.sample(frac=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(451218, 45)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to considering removing emojis and emoticons, I need to see how frequently they occur in my data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-b2730532bca6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'|'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mescape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0memojis_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#English\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0msubset_train_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'emoji'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubset_train_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'comment_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m#Emoji count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0msubset_train_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'emoji_count'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubset_train_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'emoji'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswifter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\strings.py\u001b[0m in \u001b[0;36mwrapper2\u001b[1;34m(self, pat, flags, **kwargs)\u001b[0m\n\u001b[0;32m   1754\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1755\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapper2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1756\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1757\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrap_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1758\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\strings.py\u001b[0m in \u001b[0;36mstr_findall\u001b[1;34m(arr, pat, flags)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \"\"\"\n\u001b[0;32m   1173\u001b[0m     \u001b[0mregex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1174\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_na_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\strings.py\u001b[0m in \u001b[0;36m_na_map\u001b[1;34m(f, arr, na_result, dtype)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_na_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;31m# should really _check_ for NA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mna_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\strings.py\u001b[0m in \u001b[0;36m_map\u001b[1;34m(f, arr, na_mask, na_value, dtype)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[0mconvert\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[1;31m# Reraise the exception if callable `f` got wrong number of args.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%timeit\n",
    "#extracting the emojis\n",
    "emojis_list = map(lambda x: ''.join(x.split()), UNICODE_EMO.keys())\n",
    "r = re.compile('|'.join(re.escape(p) for p in emojis_list))\n",
    "#English\n",
    "subset_train_data['emoji'] = subset_train_data['comment_text'].str.findall(r)\n",
    "#Emoji count\n",
    "subset_train_data['emoji_count'] = subset_train_data['emoji'].swifter.apply(len, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoji percentage in sample data:  0.5447 %\n"
     ]
    }
   ],
   "source": [
    "print('Emoji percentage in sample data: ', round(subset_train_data['emoji_count'].sum() / subset_train_data['emoji_count'].count() * 100,4), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the emoticons\n",
    "emoticons_list = map(lambda x: ''.join(x.split()), EMOTICONS.keys())\n",
    "r = re.compile('|'.join(re.escape(p) for p in emoticons_list))\n",
    "#English\n",
    "subset_train_data['emoticons'] = subset_train_data['comment_text'].str.findall(r)\n",
    "#Emoji count\n",
    "subset_train_data['emoticons_count'] = subset_train_data['emoticons'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoticons percentage in sample data:  4.1268 %\n"
     ]
    }
   ],
   "source": [
    "print('Emoticons percentage in sample data: ', round(subset_train_data['emoticons_count'].sum() / subset_train_data['emoticons_count'].count() * 100,4), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My sample data contains emoticons in approximately 4% of the data and emoji in .5%. Given the minimal presence of both emoji and emoticons, I'll be removing them both along with punctuation before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit\n",
    "\n",
    "subset_train_data.loc[:, 'comment_text_no_emo'] = subset_train_data.loc[:,'comment_text'].apply(lambda x: demoji.replace(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Back in 2004, there was a movie titled \"A Day Without a Mexican\" satirically depicting how the economy of California would collapse without the very people whom the \"respectable\" people loved to despise. \\'Nuff said üòé',\n",
       " \"We're trying, but there she was back on TV this week throwing around wild excuses for why she lost again.ü§ì\",\n",
       " 'Lane Community College: Climbing \\xadPoeTree.\\nAn amazing poetry group\\nMLKüíú',\n",
       " \"Mind you we just had a 15 and 17yr old rob a bank in the middle of a school day no less.  Have to wonder what the hell these parents are doing, because it isn't parenting.  \\n\\nIt isn't laying down the law and demanding kids respect their elders.  We have a bunch of kids running around acting complete criminal fools. Spanking is horrible and teaches violence? Talk to them as adults? Yeah as we can see that isn't working out to well now is it Dr Spock. \\n\\nAll of those involved are freaking kids. All of them involved knew what happened and then you get one who speaks up, why? Because he wanted the reward money! I hope they throw the book at these worthless wastes of space.  And what was that a brother who is also in jail? Yeah might want to look closer at his parents; obviously the fruit doesn't fall far from the tree. üò° Don't just hand out deals DA, seriously throw the damn book at these kids so that others might think twice before doing this again.\",\n",
       " 'Go rockies! Big win although i think they should get rid of a couple of relievers like otto and dunn, send them down to the minors and bring up some fresh talent üòÅ']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(subset_train_data[subset_train_data['emoji_count'] == 1].comment_text.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = subset_train_data[subset_train_data['emoji_count'] == 1].comment_text.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Go rockies! Big win although i think they should get rid of a couple of relievers like otto and dunn, send them down to the minors and bring up some fresh talent üòÅ']\n"
     ]
    }
   ],
   "source": [
    "print(list(convert_emojis(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TBD: \n",
    "I'm going to split each comment into a list of words before removing punctuation to preserve the integrity of contracted words, i.e. we're or won't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_train_data.loc[:,'comment_text_noaccent'] = subset_train_data.loc[:,'comment_text'].apply(lambda x: remove_accented_chars(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_train_data['compare_ifaccent'] = np.where(subset_train_data['comment_text_noaccent'] != subset_train_data['comment_text'], 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of accent characters:  4.3478 %\n"
     ]
    }
   ],
   "source": [
    "print('Number of accent characters: ', round(subset_train_data['compare_ifaccent'].sum() / subset_train_data['compare_ifaccent'].count() * 100,4), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'\\n...I just talked to God and he said I can ignore such craven egotistical nonsense.\\n\\n.\\nAs if you're the spokesman for God.   Awesome, ...now that's an ego trip.\\n.\", '(Part two of two)  O God, let all the nations praise you! in their assemblies and in searching for you in truth, in truth not in the sense of verily, but in the sense of loving the results of research.  Leviticus 25:17) has it right, do not deal unfairly (with scientists), then; but stand in fear of your god [sic].  I, the LORD, am your God and the author of all truth, no matter what you do with new findings.  Liturgy of the Word, Reading 406, Saturday of the Seventeenth Week in Ordinary Time I.']\n"
     ]
    }
   ],
   "source": [
    "print(list(subset_train_data[subset_train_data['compare_ifaccent'] == 1].comment_text_noaccent.head(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'\\n‚Ä¶I just talked to God and he said I can ignore such craven egotistical nonsense.\\n\\n.\\nAs if you're the spokesman for God.   Awesome, ‚Ä¶now that's an ego trip.\\n.\", '(Part two of two)  ‚ÄúO God, let all the nations praise you!‚Äù in their assemblies and in searching for you in truth, in truth not in the sense of verily, but in the sense of loving the results of research.  Leviticus 25:17) has it right, ‚Äúdo not deal unfairly (with scientists), then; but stand in fear of your god [sic].  I, the LORD, am your God‚Äù and the author of all truth, no matter what you do with new findings.  Liturgy of the Word, Reading 406, Saturday of the Seventeenth Week in Ordinary Time I.']\n"
     ]
    }
   ],
   "source": [
    "print(list(subset_train_data[subset_train_data['compare_ifaccent'] == 1].comment_text.head(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what can be deleted from messages field?\n",
    "-stop words, punctuation, emoji, \n",
    "\n",
    "lematization or stemming\n",
    "output of wrangling - corpus can be part of a data frame with other cleaned up columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Why removing meta-data columns? There has to be a data-science driven reason ...\n",
    "    1) can't delete columns from dataset without approval from owner of data\n",
    "    2) just because I don't see a connection doesn't mean there is one\n",
    "    3) how can use the information that we're given to create a potential feature that is a measure the level of intention - like or don't like\n",
    "\n",
    "- Null values implies values that are missing. Is it possible to have values that are missing that are not null?\n",
    "    1) cases where values are not null but can still be not good\n",
    " \n",
    "- How would we check for data quality?\n",
    "    1) max, min, range, content, etc.\n",
    "\n",
    "- There seem to be many columns with about 80% of values that are missing. How do we deal with this?\n",
    "    1) Could limit the data to the ~20% of the data with identifiers\n",
    "    2) concentrate on comment_text and response variable initially?\n",
    "    \n",
    "    a) would it make sense to compute them myself?\n",
    "\n",
    "- Do we know what columns are categorical and what columns are non-categorical? \n",
    "    1) I would say the identity labels are categorical\n",
    "    2) target is the response\n",
    "    3) what would comment_text be?\n",
    "    non-categorical predominantly - continuous metric\n",
    "    categorical - comment_text, refers to a class\n",
    "\n",
    "- Think about ways of persisting the dataset as an output of the wrangling phase, which will be an input to the next phase storytelling). (Hint ... you can use something like Pickle, or perhaps generate a CSV file)\n",
    "    1) pickle the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "emoji - encoded to a string as a token (possibility); do search on how to deal with emoji in NLP\n",
    "\n",
    "packages - nltk, spacy\n",
    "\n",
    "check if summation of identity labels across rows is equal to 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unicode remove, freq count is an option, every emoji maps to a unicode character and maybe can map it to a text entity. \n",
    "\n",
    "\n",
    "frequency stopwords, manual inspection\n",
    "think about & determine - punc, stop words, conjunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(string.punctuation)\n",
    "\n",
    "\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# split into words by white space\n",
    "subset_train_data.loc[:,'comment_text_split'] = subset_train_data.loc[:,'comment_text'].apply(lambda x: x.split())\n",
    "# remove punctuation from each word\n",
    "\n",
    "table = str.maketrans('', '', string.punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_train_data.comment_text_split.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to strip punctuation\n",
    "import string\n",
    "from string import punctuation\n",
    "\n",
    "def strip_punctuation(s):\n",
    "    return ''.join(c for c in s if c not in punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_train_data.loc[:,'comment_text_punct'] = subset_train_data.loc[:,'comment_text'].apply(lambda x: strip_punctuation(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "subset_train_data.comment_text_punct.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Count non-alphanumerical characters in sample\n",
    "count_special = 0\n",
    "for line in subset_train_data_v2['comment_text']:\n",
    "    count_special += sum(not x.isalnum() for x in line)\n",
    "print(count_special)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantify '!', \",\" ,\"\\'\" ,\";\" ,\"\\\"\", \".\", \"-\" ,\"?\" in sample data\n",
    "count = 0\n",
    "for line in subset_train_data_v2['comment_text']:\n",
    "    count += sum(1 for i in line if i in ('!', \",\" ,\"\\'\" ,\";\" ,\"\\\"\", \".\", \"-\" ,\"?\"))\n",
    "          \n",
    "print(\"Total number of punctuation characters exists in string: \")\n",
    "print(count);  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attemp to check for accented characters\n",
    "\n",
    "def count_ascii(text):\n",
    "    if text.isascii() == False:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "subset_train_data.loc[:,'comment_text_ascii'] = subset_train_data.loc[:,'comment_text'].apply(lambda x: count_accented_chars(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accented characters percentage in sample data: ', round(subset_train_data['comment_text_ascii'].sum() / subset_train_data['comment_text_ascii'].count() * 100,4), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create comment field in all lower case\n",
    "\n",
    "train_data['comment_lower'] = train_data['comment_text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['comment_lower'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of removing emojis or emoticons, we'll convert them into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert emoji and emoticons to words\n",
    "\n",
    "def convert_emojis(text):\n",
    "    for emot in UNICODE_EMO:\n",
    "        text = text.replace(emot, \"_\".join(UNICODE_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "    return text\n",
    "# Converting emoticons to words    \n",
    "def convert_emoticons(text):\n",
    "    for emot in EMOTICONS:\n",
    "        text = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").split()), text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new comment field with converted emoji and emoticon\n",
    "\n",
    "train_data['comment_transform'] = train_data['comment_lower'].apply(convert_emoticons)\n",
    "train_data['comment_transform'] = train_data['comment_lower'].apply(convert_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['comment_transform'].max()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove accented characters - convert to standardized ASCII characters\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "train_data.loc[:,'comment_text_cleaned'] = train_data.loc[:,'comment_text'].apply(lambda x: remove_accented_chars(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing contractions \n",
    "\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "train_data.loc[:,'comment_text_cleaned'] = train_data.loc[:,'comment_text_cleaned'].apply(lambda x: expand_contractions(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on results of the model, may need to go back and do more cleaning.\n",
    "\n",
    "order of complexity\n",
    "-stopwords and punctuation with NLTK\n",
    "    - be careful with apostrophe. ex - didn't -> didnt as a token (wont need to worry about contractions)\n",
    "    - if by taking care of punctuation, could take care of contractions (don't split)\n",
    "    - think about by-product\n",
    "    \n",
    "- accented words\n",
    "    - do we have a ratio or amount? is it worth while to do? \n",
    "    \n",
    "- extract count of emoji/emoticons from text\n",
    "    what proportion is it?\n",
    "    \n",
    "-lemmitization or stemming, one not both\n",
    "\n",
    "-spelling\n",
    "    -is it quantifiable; compare against a dictionary? not important for the sake of spelling but for consistency across the corpus\n",
    "    \n",
    "- split into a more manageable size to do my experimentation then apply to full data (random sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pickle file for use in the future\n",
    "\n",
    "train_data.to_pickle(dst + '/cleaned_train_data.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
